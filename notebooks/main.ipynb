{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f30daf",
   "metadata": {},
   "source": [
    "# Neural Numpy\n",
    "This notebook will go through using the neural numpy module we have developed, found in ../src/neural_numpy. To start off, this project is managed using the awesome package manager [uv](https://docs.astral.sh/uv/). Firstly, ensure that you have installed uv and run `uv sync` to create the venv. After this, select the venv as your kernel in the jupyter notebook. This should now allow you to start importing the libraries needed, as well as the neural-numpy module.\n",
    "\n",
    "## The network config\n",
    "The network has a top level classed creatively called `network`. A network consists of layers, either dense or activation layers, and it handles orchestrating the layers, such as when running forward, backpropagating or training the network. This network can be built manually, or alternatively using the builder class, which is the preferred method. The builder class takes in a config, and builds a network using the parameters given in the config. For the purpose of this project we have mostly used wandb for logging, and therefore a wandb config dictionary can be passed to the builder (However, a method for normal python parameters also exists) Let's start by importing some of the libraries needed to run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd09c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from neural_numpy.builder import NetworkBuilder\n",
    "from neural_numpy.loss import CategoricalCrossEntropy\n",
    "from neural_numpy.optimizer import ADAM, SGD\n",
    "from data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a4557",
   "metadata": {},
   "source": [
    "## Sweep time\n",
    "As a starting point we needed to figure out what hyperparameters would be optimal for out network. An obvious way to do this was using wandb hyperparameter sweeps. As a starting point we define a sweep configuration, which was used to determine the optimal hyperparameters. Bayesian optimization was used to speed up the tuning process as opposed to for example using random sweeps. We chose a value of 50 epochs, which was enough to determine what models yielded good results without taking too long to train. We tested a variety of batch size and number of hidden units, which we chose to cap at 1024, as the network took an extreme amount of time to train when going up to for instance 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_acc\",\n",
    "        \"goal\": \"maximize\",\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"value\": 50},  # Fixed value\n",
    "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "        \"learning_rate\": {\n",
    "            \"max\": 0.01,\n",
    "            \"min\": 0.0001,\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "        },\n",
    "        \"hidden_layers\": {\"values\": [2, 3, 4, 5]},\n",
    "        \"hidden_units\": {\"values\": [64, 128, 256, 512, 1024]},\n",
    "        \"activation\": {\"values\": [\"ReLU\", \"Tanh\", \"Sigmoid\"]},\n",
    "        \"weight_initializer\": {\"values\": [\"He\", \"Xavier\"]},\n",
    "        \"optimizer\": {\"values\": [\"adam\", \"sgd\"]},\n",
    "        \"weight_decay\": {\"values\": [0.0, 1e-3, 1e-4]},\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b042ab",
   "metadata": {},
   "source": [
    "After this we imported a dataset, in this case CIFAR10, and split the traing data into a training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34576f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X_train, y_train, X_test, y_test = DataLoader.load_cifar10(\n",
    "    normalize=True, flatten=True, one_hot=True\n",
    ")\n",
    "\n",
    "# Split data\n",
    "val_split = 0.2\n",
    "split_idx = int(X_train.shape[0] * (1 - val_split))\n",
    "X_val = X_train[split_idx:]\n",
    "y_val = y_train[split_idx:]\n",
    "X_train_split = X_train[:split_idx]\n",
    "y_train_split = y_train[:split_idx]\n",
    "\n",
    "# Determine input dimensions\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2aa910",
   "metadata": {},
   "source": [
    "We define this training data once so that we don't have to import it at every training run. This leads us to the actual training function, which runs for every training run in the sweep:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    #Initialize Wandb\n",
    "    with wandb.init() as run:\n",
    "        # Use the config from the sweep\n",
    "        config = wandb.config\n",
    "\n",
    "        # Initialize builder\n",
    "        builder = NetworkBuilder()\n",
    "\n",
    "        # Build network from the wandb config\n",
    "        network = builder.build_from_wandb(\n",
    "            input_size=input_dim, output_size=num_classes, config=config\n",
    "        )\n",
    "        \n",
    "        # As the optimizer is passed to the training run, this has to be defined externally\n",
    "        if config.optimizer.lower() == \"adam\":\n",
    "            optimizer = ADAM(\n",
    "                learning_rate=config.learning_rate,\n",
    "                weight_decay=getattr(config, \"weight_decay\", 0.0),\n",
    "            )\n",
    "        elif config.optimizer.lower() == \"sgd\":\n",
    "            optimizer = SGD(\n",
    "                learning_rate=config.learning_rate,\n",
    "                weight_decay=getattr(config, \"weight_decay\", 0.0),\n",
    "            )\n",
    "        # As does the loss function\n",
    "        loss_fn = CategoricalCrossEntropy()\n",
    "\n",
    "        #Finally, pass the parameters to the training run\n",
    "        network.train(\n",
    "            X=X_train_split,\n",
    "            y=y_train_split,\n",
    "            X_val=X_val,\n",
    "            y_val=y_val,\n",
    "            loss_function=loss_fn,\n",
    "            epochs=config.epochs,\n",
    "            optimizer=optimizer,\n",
    "            batch_size=config.batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f8a43",
   "metadata": {},
   "source": [
    "Now we can finally run the sweep, which we set to run overnight on one of our computers with a powerful CPU (No GPU-acceleration sadly). Warning that if you actualy run this it will take quite a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to WandB\n",
    "wandb.login()\n",
    "\n",
    "# Initialize the sweep with the config and a project name\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"sweepalicious\")\n",
    "\n",
    "# Start the sweep agent with the train_sweep as the callback. This is the one we ran overnight\n",
    "wandb.agent(sweep_id, function=train_sweep, count=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c30258",
   "metadata": {},
   "source": [
    "## Training and measuring the network\n",
    "Now that we had found some promising hyperparameters using the hyperparameter sweep, we went on to test out a candidate. This was done quite similairly to the last run. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
