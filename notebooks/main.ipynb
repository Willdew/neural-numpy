{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f30daf",
   "metadata": {},
   "source": [
    "# Neural Numpy\n",
    "This notebook will go through using the neural numpy module we have developed, found in ../src/neural_numpy. To start off, this project is managed using the awesome package manager [uv](https://docs.astral.sh/uv/). Firstly, ensure that you have installed uv and run `uv sync` to create the venv. After this, select the venv as your kernel in the jupyter notebook. This should now allow you to start importing the libraries needed, as well as the neural-numpy module.\n",
    "\n",
    "## The network config\n",
    "The network has a top level classed creatively called `network`. A network consists of layers, either dense or activation layers, and it handles orchestrating the layers, such as when running forward, backpropagating or training the network. This network can be built manually, or alternatively using the builder class, which is the preferred method. The builder class takes in a config, and builds a network using the parameters given in the config. For the purpose of this project we have mostly used wandb for logging, and therefore a wandb config dictionary can be passed to the builder (However, a method for normal python parameters also exists) Let's start by importing some of the libraries needed to run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd09c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from neural_numpy.builder import NetworkBuilder\n",
    "from neural_numpy.loss import CategoricalCrossEntropy\n",
    "from neural_numpy.optimizer import ADAM, SGD\n",
    "from data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a4557",
   "metadata": {},
   "source": [
    "## Hyperparameter sweeps\n",
    "As a starting point we needed to figure out what hyperparameters would be optimal for out network. An obvious way to do this was using wandb hyperparameter sweeps. As a starting point we define a sweep configuration, which was used to determine the optimal hyperparameters. Bayesian optimization was used to speed up the tuning process as opposed to for example using random sweeps. We chose a value of 50 epochs, which was enough to determine what models yielded good results without taking too long to train. We tested a variety of batch size and number of hidden units, which we chose to cap at 1024, as the network took an extreme amount of time to train when going up to for instance 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_acc\",\n",
    "        \"goal\": \"maximize\",\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"value\": 50},  # Fixed value\n",
    "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "        \"learning_rate\": {\n",
    "            \"max\": 0.01,\n",
    "            \"min\": 0.0001,\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "        },\n",
    "        \"hidden_layers\": {\"values\": [2, 3, 4, 5]},\n",
    "        \"hidden_units\": {\"values\": [64, 128, 256, 512, 1024]},\n",
    "        \"activation\": {\"values\": [\"ReLU\", \"Tanh\", \"Sigmoid\"]},\n",
    "        \"weight_initializer\": {\"values\": [\"He\", \"Xavier\"]},\n",
    "        \"optimizer\": {\"values\": [\"adam\", \"sgd\"]},\n",
    "        \"weight_decay\": {\"values\": [0.0, 1e-3, 1e-4]},\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b042ab",
   "metadata": {},
   "source": [
    "After this we imported a dataset, in this case CIFAR10, and split the traing data into a training and validation set. Note that to get out mnist results, `Dataloader.load_cifar10` can simply be replaced with `Dataloader.load_mnist`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34576f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X_train, y_train, X_test, y_test = DataLoader.load_cifar10(\n",
    "    normalize=True, flatten=True, one_hot=True\n",
    ")\n",
    "\n",
    "# Split data\n",
    "val_split = 0.2\n",
    "split_idx = int(X_train.shape[0] * (1 - val_split))\n",
    "X_val = X_train[split_idx:]\n",
    "y_val = y_train[split_idx:]\n",
    "X_train_split = X_train[:split_idx]\n",
    "y_train_split = y_train[:split_idx]\n",
    "\n",
    "# Determine input dimensions\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2aa910",
   "metadata": {},
   "source": [
    "We define this training data once so that we don't have to import it at every training run. This leads us to the actual training function, which runs for every training run in the sweep:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    #Initialize Wandb\n",
    "    with wandb.init() as run:\n",
    "        # Use the config from the sweep\n",
    "        config = wandb.config\n",
    "\n",
    "        # Initialize builder\n",
    "        builder = NetworkBuilder()\n",
    "\n",
    "        # Build network from the wandb config\n",
    "        network = builder.build_from_wandb(\n",
    "            input_size=input_dim, output_size=num_classes, config=config\n",
    "        )\n",
    "        \n",
    "        # As the optimizer is passed to the training run, this has to be defined externally\n",
    "        if config.optimizer.lower() == \"adam\":\n",
    "            optimizer = ADAM(\n",
    "                learning_rate=config.learning_rate,\n",
    "                weight_decay=getattr(config, \"weight_decay\", 0.0),\n",
    "            )\n",
    "        elif config.optimizer.lower() == \"sgd\":\n",
    "            optimizer = SGD(\n",
    "                learning_rate=config.learning_rate,\n",
    "                weight_decay=getattr(config, \"weight_decay\", 0.0),\n",
    "            )\n",
    "        # As does the loss function\n",
    "        loss_fn = CategoricalCrossEntropy()\n",
    "\n",
    "        #Finally, pass the parameters to the training run\n",
    "        network.train(\n",
    "            X=X_train_split,\n",
    "            y=y_train_split,\n",
    "            X_val=X_val,\n",
    "            y_val=y_val,\n",
    "            loss_function=loss_fn,\n",
    "            epochs=config.epochs,\n",
    "            optimizer=optimizer,\n",
    "            batch_size=config.batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f8a43",
   "metadata": {},
   "source": [
    "Now we can finally run the sweep, which we set to run overnight on one of our computers with a powerful CPU (No GPU-acceleration sadly). Warning that if you actualy run this it will take quite a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to WandB\n",
    "wandb.login()\n",
    "\n",
    "# Initialize the sweep with the config and a project name\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"sweepalicious\")\n",
    "\n",
    "# Start the sweep agent with the train_sweep as the callback. This is the one we ran overnight\n",
    "wandb.agent(sweep_id, function=train_sweep, count=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c30258",
   "metadata": {},
   "source": [
    "## Training and measuring the network\n",
    "Now that we had found some promising hyperparameters using the hyperparameter sweep, we went on to test out a candidate. This was done quite similairly to the last run, however this time with a specific config. Like before, we started out importing the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from rich import box, print\n",
    "from rich.table import Table\n",
    "\n",
    "import wandb\n",
    "from data import DataLoader\n",
    "from neural_numpy.builder import NetworkBuilder\n",
    "from neural_numpy.confusion_matrix import confusion_matrix\n",
    "from neural_numpy.loss import CategoricalCrossEntropy\n",
    "from neural_numpy.optimizer import ADAM, SGD\n",
    "\n",
    "X_train, y_train, X_test, y_test = DataLoader.load_cifar10( # Can be replaced by mnist\n",
    "    normalize=True, flatten=True, one_hot=True\n",
    ")\n",
    "print(\"[bold green]Data Loaded:[/bold green] CIFAR-10 Dataset\")\n",
    "\n",
    "val_split = 0.2\n",
    "split_idx = int(X_train.shape[0] * (1 - val_split))\n",
    "\n",
    "# Validation data\n",
    "X_val = X_train[split_idx:]\n",
    "y_val = y_train[split_idx:]\n",
    "\n",
    "# Training data\n",
    "X = X_train[:split_idx]\n",
    "y = y_train[:split_idx]\n",
    "\n",
    "print(f\"[bold green]Training Set:[/bold green] {X.shape[0]} samples\")\n",
    "print(f\"[bold green]Validation Set:[/bold green] {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c53b19",
   "metadata": {},
   "source": [
    "After this we define the config from one of the promising sweeps. Note that for the mnist set this config was used:\n",
    "```\n",
    "config={\n",
    "    \"epochs\": 100,\n",
    "    \"learning_rate\": 0.000992898461755694,\n",
    "    \"batch_size\": 32,\n",
    "    # Architecture\n",
    "    \"hidden_layers\": 4,\n",
    "    \"hidden_units\": 512,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"activation\": \"ReLU\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"weight_initializer\": \"He\",\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7682d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"azure\",\n",
    "    config={\n",
    "        \"epochs\": 4,\n",
    "        \"learning_rate\": 0.00909273148274152,\n",
    "        \"batch_size\": 64,\n",
    "        \"hidden_layers\": 3,\n",
    "        \"hidden_units\": 512,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"activation\": \"ReLU\",\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"weight_initializer\": \"Xavier\",\n",
    "    },\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782ad62",
   "metadata": {},
   "source": [
    "We build the network with the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "builder = NetworkBuilder()\n",
    "network = builder.build_from_wandb(\n",
    "    input_size=input_dim, output_size=num_classes, config=config\n",
    ")\n",
    "\n",
    "if config.optimizer.lower() == \"adam\":\n",
    "    optimizer = ADAM(\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=getattr(config, \"weight_decay\", 0.0),\n",
    "    )\n",
    "elif config.optimizer.lower() == \"sgd\":\n",
    "    optimizer = SGD(\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=getattr(config, \"weight_decay\", 0.0),\n",
    "    )\n",
    "loss_fn = CategoricalCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e7a0f",
   "metadata": {},
   "source": [
    "We now train the network with our config from before (This might take a while) (We mostly use the terminal, but this should also print interactively in the jupyter terminal!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    loss_function=loss_fn,\n",
    "    epochs=config.epochs,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=config.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d6b82",
   "metadata": {},
   "source": [
    "We can now finally determine the accuracy of the neural network on the test data, and print out the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure network predictions against actual values \n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "val_pred = network.forward(X_test)\n",
    "y_pred = np.argmax(val_pred, axis=1)\n",
    "val_acc_train = np.mean(np.argmax(val_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(f\"[bold green]Test Accuracy: {val_acc_train:.1%}\")\n",
    "\n",
    "# Print class names of confusion matrix - this could probably be done in a better way hehe\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "num_classes = cm.shape[0]\n",
    "\n",
    "# Make a rich table\n",
    "table = Table(title=\"Confusion Matrix\", box=box.ROUNDED, show_lines=True)\n",
    "table.add_column(\"True\\\\Pred\", style=\"dim\", width=12)\n",
    "\n",
    "# Construct table \n",
    "for i in range(num_classes):\n",
    "    table.add_column(class_names[i], justify=\"right\")\n",
    "\n",
    "for i in range(num_classes):\n",
    "    row_data: List = []\n",
    "    row_data.append(class_names[i])\n",
    "    for j, val in enumerate(cm[i]):\n",
    "        if j == i:\n",
    "            row_data.append(\"[bold green]\" + str(int(val)))\n",
    "        else:\n",
    "            row_data.append(str(int(val)))\n",
    "    table.add_row(*row_data)\n",
    "print(table)\n",
    "\n",
    "# Finish run :)\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
